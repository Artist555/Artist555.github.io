{"meta":{"title":"Artist555 Blog","subtitle":null,"description":null,"author":"Artist555","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"《机器学习》-周志华-第一章","slug":"first","date":"2019-07-03T13:20:49.000Z","updated":"2019-07-03T15:59:39.680Z","comments":true,"path":"2019/07/03/first/","link":"","permalink":"http://yoursite.com/2019/07/03/first/","excerpt":"","text":"第1章 绪论1.1 引言机器学习致力于研究如何通过计算的手段，利用经验来玫善系统自身的性能。机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm）。 有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型；在面对新的情况时（例如看到一个没剖开的西瓜），模型会给我们提供相应的判断（例如好瓜）。如果说计算机科学是研究关于”算法”的学问，那么类似的，可以说机器学习是研究关于”学习算法”的学问。 此书用”模型”泛指从数据中学得的结果。有文献用”模型”指全局性结果（例如一棵决策树），而用”模式”指局部性结呆（例如A条规则）。 1.2 基本术语数据集（data set）：数据记录的集合示例（instance）/ 样本（sample）：一个事件或对象的描述属性（attribute）/ 特征（feature）：反应事件或对象在某方面的表现或性质的事项属性值（attribute value）：属性上的取值属性空间（attribute space）/ 样本空间（sample space）/ 输入空间：属性张成的空间特征向量（feature vector）：属性空间里坐标位置对应的示例学习（learning）/ 训练（training）：从数据中学得模型的过程训练数据（training data）：训练过程中使用的数据训练样本（training sample）：训练数据中的样本训练集（training set）：训练样本组成的集合假设（hypothesis）：学得模型对应了关于数据的某种潜在的规律真相 / 真实（ground-truth）：潜在规律自身学习器（learner）：此书中有时模型的别称标记（label）：关于示例结果的信息样例（example）：拥有标记信息的示例标记空间（label space）/ 输出空间：所有标记的集合分类（classification）：预测是离散值的学习任务回归（regression）：预测是连续值的学习任务正类（positive class）：只设计两个类别的“二分类（binary classification）”任务中的其中一类反类（negative class）：另一个类多分类（muli-class classification）任务：涉及多个类别的任务测试（testing）：学的模型后，使用其进行预测的过程测试样本（teating sample）：预测的样本簇（cluster）：训练集中分成的若干组泛化（generalization）能力：学得模型适用于新样本的能力 1.3 假设空间归纳（induction）：从特殊到一般的“泛化”过程，即从具体的事实归纳出一般性规律 演绎（deduction）：从一般到“特化”过程，即从基础原理推演出具体状况 归纳学习 / 概念学习 / 概念形成：归纳学习有狭义与广义之分，广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念（concept） 我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”（fit）的假设，即能够将训练集中的瓜判断正确的假设。 版本空间（version space）：可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合” 1.4 归纳偏好归纳偏好（inductive bias）/ 偏好：机器学习算法在学习过程中对某种类型假设的偏好 任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上”等效”的假设所迷惑，而无法产生确定的学习结果。 对有限个样本点组成的训练集，存在着很多条曲线与其一致，我们的学习算法必须有某种偏好，才能产出它认为”正确”的模型。 “奥卡姆剃刀”（Occam’s razor）是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个” 事实上，归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。 无论学习算法a多聪明、学习算法b多笨拙，它们的期望性能竟然相同！这就是“没有免费的午餐”定理（No Free Lunch Theorem，简称NFL定理）。 很多时候，我们只关注自己正在试图解决的问题（例如某个具体应用任务），希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。","categories":[],"tags":[]}]}